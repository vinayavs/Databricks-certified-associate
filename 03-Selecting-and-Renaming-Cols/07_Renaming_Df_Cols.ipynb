{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4796235a-feb0-49b7-a6c4-294be770dc53",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "There are multiple ways to rename Spark Data Frame Columns or Expressions.\n",
    "* We can rename column or expression using `alias` as part of `select`\n",
    "* We can add or rename column or expression using `withColumn` on top of Data Frame.\n",
    "* We can rename one column at a time using `withColumnRenamed` on top of Data Frame.\n",
    "* We typically use `withColumn` to perform row level transformations and then to provide a name to the result. If we provide the same name as existing column, then the column will be replaced with new one.\n",
    "* If we want to just rename the column then it is better to use `withColumnRenamed`.\n",
    "* If we want to apply any transformation, we need to either use `select` or `withColumn`\n",
    "* We can rename bunch of columns using `toDF`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f305c34d-6d05-4149-aed1-fead15a99a41",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"./01_Create_Df_Template\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a92715e-9e89-4f34-8179-b6a32c4bd81d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+------------+--------------------+\n| id|first_name|   last_name|           full_name|\n+---+----------+------------+--------------------+\n|  1|    Corrie|Van den Oord|Corrie, Van den Oord|\n|  2|  Nikolaus|     Brewitt|   Nikolaus, Brewitt|\n|  3|    Orelie|      Penney|      Orelie, Penney|\n|  4|     Ashby|    Maddocks|     Ashby, Maddocks|\n|  5|      Kurt|        Rome|          Kurt, Rome|\n+---+----------+------------+--------------------+\n\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.databricks.v1+bamboolib_hint": "{\"pd.DataFrames\": [], \"version\": \"0.0.1\"}",
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+------------+--------------------+--------------------+-------+-----------+-----------+-------------+-------------------+\n| id|first_name|   last_name|               email|       phone_numbers|courses|is_customer|amount_paid|customer_from|    last_updated_ts|\n+---+----------+------------+--------------------+--------------------+-------+-----------+-----------+-------------+-------------------+\n|  1|    Corrie|Van den Oord|cvandenoord0@etsy...|{+1 234 567 8901,...| [1, 2]|       true|    1000.55|   2021-01-15|2021-02-10 01:15:00|\n|  2|  Nikolaus|     Brewitt|nbrewitt1@dailyma...|{+1 234 567 8923,...|    [3]|       true|      900.0|   2021-02-14|2021-02-18 03:33:00|\n|  3|    Orelie|      Penney|openney2@vistapri...|{+1 714 512 9752,...| [2, 4]|       true|     850.55|   2021-01-21|2021-03-15 15:16:55|\n|  4|     Ashby|    Maddocks|  amaddocks3@home.pl|        {null, null}|     []|      false|        NaN|         null|2021-04-10 17:45:30|\n|  5|      Kurt|        Rome|krome4@shutterfly...|{+1 817 934 7142,...|     []|      false|        NaN|         null|2021-04-02 00:55:18|\n+---+----------+------------+--------------------+--------------------+-------+-----------+-----------+-------------+-------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Naming Derived Cols Using With Column\n",
    "# Using Concat and lit and select\n",
    "from pyspark.sql.functions import col, concat, lit\n",
    "usersDf.select(\n",
    "    \"id\", \"first_name\", \"last_name\",\n",
    "    (concat(col(\"first_name\"), lit(\", \"), col(\"last_name\"))).alias(\"full_name\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34af8848-d83e-4849-bafe-af8e85939705",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+------------+--------------------+\n| id|first_name|   last_name|           full_name|\n+---+----------+------------+--------------------+\n|  1|    Corrie|Van den Oord|Corrie, Van den Oord|\n|  2|  Nikolaus|     Brewitt|   Nikolaus, Brewitt|\n|  3|    Orelie|      Penney|      Orelie, Penney|\n|  4|     Ashby|    Maddocks|     Ashby, Maddocks|\n|  5|      Kurt|        Rome|          Kurt, Rome|\n+---+----------+------------+--------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Using withColumn syntax\n",
    "usersDf.select(\"id\", \"first_name\", \"last_name\") \\\n",
    "    .withColumn(\"full_name\", concat(col(\"first_name\"), lit(\", \"), col(\"last_name\"))) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2571517-9622-491d-ad1d-721f4622baa3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method withColumn in module pyspark.sql.dataframe:\n\nwithColumn(colName: str, col: pyspark.sql.column.Column) -> 'DataFrame' method of pyspark.sql.dataframe.DataFrame instance\n    Returns a new :class:`DataFrame` by adding a column or replacing the\n    existing column that has the same name.\n    \n    The column expression must be an expression over this :class:`DataFrame`; attempting to add\n    a column from some other :class:`DataFrame` will raise an error.\n    \n    .. versionadded:: 1.3.0\n    \n    .. versionchanged:: 3.4.0\n        Support Spark Connect.\n    \n    Parameters\n    ----------\n    colName : str\n        string, name of the new column.\n    col : :class:`Column`\n        a :class:`Column` expression for the new column.\n    \n    Returns\n    -------\n    :class:`DataFrame`\n        DataFrame with new or replaced column.\n    \n    Notes\n    -----\n    This method introduces a projection internally. Therefore, calling it multiple\n    times, for instance, via loops in order to add multiple columns can generate big\n    plans which can cause performance issues and even `StackOverflowException`.\n    To avoid this, use :func:`select` with multiple columns at once.\n    \n    Examples\n    --------\n    >>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n    >>> df.withColumn('age2', df.age + 2).show()\n    +---+-----+----+\n    |age| name|age2|\n    +---+-----+----+\n    |  2|Alice|   4|\n    |  5|  Bob|   7|\n    +---+-----+----+\n\n"
     ]
    }
   ],
   "source": [
    "help(usersDf.withColumn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e5b6907-5003-4b25-8aa6-2c22aaac8480",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+------------+\n| id|courses|course_count|\n+---+-------+------------+\n|  1| [1, 2]|           2|\n|  2|    [3]|           1|\n|  3| [2, 4]|           2|\n|  4|     []|           0|\n|  5|     []|           0|\n+---+-------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import size\n",
    "usersDf.select(\"id\", \"courses\") \\\n",
    "    .withColumn(\"course_count\", size(\"courses\")) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb753f11-63cf-4d10-9c33-a1402616e96b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method withColumnRenamed in module pyspark.sql.dataframe:\n\nwithColumnRenamed(existing: str, new: str) -> 'DataFrame' method of pyspark.sql.dataframe.DataFrame instance\n    Returns a new :class:`DataFrame` by renaming an existing column.\n    This is a no-op if the schema doesn't contain the given column name.\n    \n    .. versionadded:: 1.3.0\n    \n    .. versionchanged:: 3.4.0\n        Support Spark Connect.\n    \n    Parameters\n    ----------\n    existing : str\n        string, name of the existing column to rename.\n    new : str\n        string, new name of the column.\n    \n    Returns\n    -------\n    :class:`DataFrame`\n        DataFrame with renamed column.\n    \n    Examples\n    --------\n    >>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n    >>> df.withColumnRenamed('age', 'age2').show()\n    +----+-----+\n    |age2| name|\n    +----+-----+\n    |   2|Alice|\n    |   5|  Bob|\n    +----+-----+\n\n"
     ]
    }
   ],
   "source": [
    "help(usersDf.withColumnRenamed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12f9d0d6-e6c1-4dc5-a945-9dcbd8d603b3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------------+\n|user_id|user_first_name|\n+-------+---------------+\n|      1|         Corrie|\n|      2|       Nikolaus|\n|      3|         Orelie|\n|      4|          Ashby|\n|      5|           Kurt|\n+-------+---------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Renaming Cols withColumnRenamed\n",
    "# If we want to rename bunch of cols the we need to use toDF\n",
    "usersDf.select(\"id\", \"first_name\") \\\n",
    "    .withColumnRenamed(\"id\", \"user_id\") \\\n",
    "    .withColumnRenamed(\"first_name\", \"user_first_name\") \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3bdc4111-9a36-4c96-940f-2989a74b89b5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------------+--------------+--------------------+\n|user_id|user_first_name|user_last_name|      user_full_name|\n+-------+---------------+--------------+--------------------+\n|      1|         Corrie|  Van den Oord|Corrie, Van den Oord|\n|      2|       Nikolaus|       Brewitt|   Nikolaus, Brewitt|\n|      3|         Orelie|        Penney|      Orelie, Penney|\n|      4|          Ashby|      Maddocks|     Ashby, Maddocks|\n|      5|           Kurt|          Rome|          Kurt, Rome|\n+-------+---------------+--------------+--------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Renaming using alais\n",
    "from pyspark.sql.functions import lit, col, concat\n",
    "\n",
    "usersDf.select(\n",
    "    col(\"id\").alias(\"user_id\"), \n",
    "    col(\"first_name\").alias(\"user_first_name\"),\n",
    "    col(\"last_name\").alias(\"user_last_name\"),\n",
    "    concat(col(\"first_name\"), lit(\", \"), col(\"last_name\")).alias(\"user_full_name\")) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a308a44-486e-41e9-b871-51de899a1118",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------------+--------------+\n|user_id|user_first_name|user_last_name|\n+-------+---------------+--------------+\n|      1|         Corrie|  Van den Oord|\n|      2|       Nikolaus|       Brewitt|\n|      3|         Orelie|        Penney|\n|      4|          Ashby|      Maddocks|\n|      5|           Kurt|          Rome|\n+-------+---------------+--------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Renaming and Reordering mulitple Cols\n",
    "required_cols=[\"id\", \"first_name\", \"last_name\"]\n",
    "tgt_cols=[\"user_id\",\"user_first_name\", \"user_last_name\"]\n",
    "\n",
    "usersDf.select(required_cols).toDF(*tgt_cols).show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "07_Renaming_Df_Cols",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
