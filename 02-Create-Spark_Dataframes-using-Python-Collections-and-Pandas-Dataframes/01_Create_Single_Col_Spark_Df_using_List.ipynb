{"cells":[{"cell_type":"code","source":["roll_num_list=[10,20,23,25]"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"c8ab93a1-c6eb-4df1-bde5-fb416727e42d","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["type(roll_num_list)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"bdc70939-6ecc-4b07-8e44-6ffeffc052de","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Out[8]: list"]}],"execution_count":0},{"cell_type":"code","source":["spark"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"9ed10c34-b026-453f-8f7e-cbdfc901e91d","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"\n            <div>\n                <p><b>SparkSession - hive</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"/?o=2243488185639557#setting/sparkui/0715-164903-zv1jz8o7/driver-2787497432188349933\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.3.2</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[8]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>Databricks Shell</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        ","textData":null,"removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"htmlSandbox","arguments":{}}},"output_type":"display_data","data":{"text/html":["\n","            <div>\n","                <p><b>SparkSession - hive</b></p>\n","                \n","        <div>\n","            <p><b>SparkContext</b></p>\n","\n","            <p><a href=\"/?o=2243488185639557#setting/sparkui/0715-164903-zv1jz8o7/driver-2787497432188349933\">Spark UI</a></p>\n","\n","            <dl>\n","              <dt>Version</dt>\n","                <dd><code>v3.3.2</code></dd>\n","              <dt>Master</dt>\n","                <dd><code>local[8]</code></dd>\n","              <dt>AppName</dt>\n","                <dd><code>Databricks Shell</code></dd>\n","            </dl>\n","        </div>\n","        \n","            </div>\n","        "]}}],"execution_count":0},{"cell_type":"code","source":["help(spark.createDataFrame)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"5b4999a2-4aa6-4750-b930-07a7b9976e73","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Help on method createDataFrame in module pyspark.sql.session:\n\ncreateDataFrame(data: Union[pyspark.rdd.RDD[Any], Iterable[Any], ForwardRef('PandasDataFrameLike'), ForwardRef('ArrayLike')], schema: Union[pyspark.sql.types.AtomicType, pyspark.sql.types.StructType, str, NoneType] = None, samplingRatio: Optional[float] = None, verifySchema: bool = True) -> pyspark.sql.dataframe.DataFrame method of pyspark.sql.session.SparkSession instance\n    Creates a :class:`DataFrame` from an :class:`RDD`, a list, a :class:`pandas.DataFrame`\n    or a :class:`numpy.ndarray`.\n    \n    .. versionadded:: 2.0.0\n    \n    .. versionchanged:: 3.4.0\n        Support Spark Connect.\n    \n    Parameters\n    ----------\n    data : :class:`RDD` or iterable\n        an RDD of any kind of SQL data representation (:class:`Row`,\n        :class:`tuple`, ``int``, ``boolean``, etc.), or :class:`list`,\n        :class:`pandas.DataFrame` or :class:`numpy.ndarray`.\n    schema : :class:`pyspark.sql.types.DataType`, str or list, optional\n        a :class:`pyspark.sql.types.DataType` or a datatype string or a list of\n        column names, default is None.  The data type string format equals to\n        :class:`pyspark.sql.types.DataType.simpleString`, except that top level struct type can\n        omit the ``struct<>``.\n    \n        When ``schema`` is a list of column names, the type of each column\n        will be inferred from ``data``.\n    \n        When ``schema`` is ``None``, it will try to infer the schema (column names and types)\n        from ``data``, which should be an RDD of either :class:`Row`,\n        :class:`namedtuple`, or :class:`dict`.\n    \n        When ``schema`` is :class:`pyspark.sql.types.DataType` or a datatype string, it must\n        match the real data, or an exception will be thrown at runtime. If the given schema is\n        not :class:`pyspark.sql.types.StructType`, it will be wrapped into a\n        :class:`pyspark.sql.types.StructType` as its only field, and the field name will be\n        \"value\". Each record will also be wrapped into a tuple, which can be converted to row\n        later.\n    samplingRatio : float, optional\n        the sample ratio of rows used for inferring. The first few rows will be used\n        if ``samplingRatio`` is ``None``.\n    verifySchema : bool, optional\n        verify data types of every row against schema. Enabled by default.\n    \n        .. versionadded:: 2.1.0\n    \n    Returns\n    -------\n    :class:`DataFrame`\n    \n    Notes\n    -----\n    Usage with `spark.sql.execution.arrow.pyspark.enabled=True` is experimental.\n    \n    Examples\n    --------\n    Create a DataFrame from a list of tuples.\n    \n    >>> spark.createDataFrame([('Alice', 1)]).collect()\n    [Row(_1='Alice', _2=1)]\n    >>> spark.createDataFrame([('Alice', 1)], ['name', 'age']).collect()\n    [Row(name='Alice', age=1)]\n    \n    Create a DataFrame from a list of dictionaries\n    \n    >>> d = [{'name': 'Alice', 'age': 1}]\n    >>> spark.createDataFrame(d).collect()\n    [Row(age=1, name='Alice')]\n    \n    Create a DataFrame from an RDD.\n    \n    >>> rdd = spark.sparkContext.parallelize([('Alice', 1)])\n    >>> spark.createDataFrame(rdd).collect()\n    [Row(_1='Alice', _2=1)]\n    >>> df = spark.createDataFrame(rdd, ['name', 'age'])\n    >>> df.collect()\n    [Row(name='Alice', age=1)]\n    \n    Create a DataFrame from Row instances.\n    \n    >>> from pyspark.sql import Row\n    >>> Person = Row('name', 'age')\n    >>> person = rdd.map(lambda r: Person(*r))\n    >>> df2 = spark.createDataFrame(person)\n    >>> df2.collect()\n    [Row(name='Alice', age=1)]\n    \n    Create a DataFrame with the explicit schema specified.\n    \n    >>> from pyspark.sql.types import *\n    >>> schema = StructType([\n    ...    StructField(\"name\", StringType(), True),\n    ...    StructField(\"age\", IntegerType(), True)])\n    >>> df3 = spark.createDataFrame(rdd, schema)\n    >>> df3.collect()\n    [Row(name='Alice', age=1)]\n    \n    Create a DataFrame from a pandas DataFrame.\n    \n    >>> spark.createDataFrame(df.toPandas()).collect()  # doctest: +SKIP\n    [Row(name='Alice', age=1)]\n    >>> spark.createDataFrame(pandas.DataFrame([[1, 2]])).collect()  # doctest: +SKIP\n    [Row(0=1, 1=2)]\n    \n    Create  a DataFrame from an RDD with the schema in DDL formatted string.\n    \n    >>> spark.createDataFrame(rdd, \"a: string, b: int\").collect()\n    [Row(a='Alice', b=1)]\n    >>> rdd = rdd.map(lambda row: row[1])\n    >>> spark.createDataFrame(rdd, \"int\").collect()\n    [Row(value=1)]\n    \n    When the type is unmatched, it throws an exception.\n    \n    >>> spark.createDataFrame(rdd, \"boolean\").collect() # doctest: +IGNORE_EXCEPTION_DETAIL\n    Traceback (most recent call last):\n        ...\n    Py4JJavaError: ...\n\n"]}],"execution_count":0},{"cell_type":"code","source":["spark.createDataFrame(roll_num_list)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"ca640dac-f100-4711-9216-afb98c53d316","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)\nFile \u001B[0;32m<command-930677172032680>:1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mspark\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcreateDataFrame\u001B[49m\u001B[43m(\u001B[49m\u001B[43mroll_num_list\u001B[49m\u001B[43m)\u001B[49m\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     51\u001B[0m     )\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1216\u001B[0m, in \u001B[0;36mSparkSession.createDataFrame\u001B[0;34m(self, data, schema, samplingRatio, verifySchema)\u001B[0m\n\u001B[1;32m   1211\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_pandas \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data, pd\u001B[38;5;241m.\u001B[39mDataFrame):\n\u001B[1;32m   1212\u001B[0m     \u001B[38;5;66;03m# Create a DataFrame from pandas DataFrame.\u001B[39;00m\n\u001B[1;32m   1213\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m(SparkSession, \u001B[38;5;28mself\u001B[39m)\u001B[38;5;241m.\u001B[39mcreateDataFrame(  \u001B[38;5;66;03m# type: ignore[call-overload]\u001B[39;00m\n\u001B[1;32m   1214\u001B[0m         data, schema, samplingRatio, verifySchema\n\u001B[1;32m   1215\u001B[0m     )\n\u001B[0;32m-> 1216\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_create_dataframe\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1217\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mschema\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msamplingRatio\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mverifySchema\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# type: ignore[arg-type]\u001B[39;49;00m\n\u001B[1;32m   1218\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1266\u001B[0m, in \u001B[0;36mSparkSession._create_dataframe\u001B[0;34m(self, data, schema, samplingRatio, verifySchema)\u001B[0m\n\u001B[1;32m   1264\u001B[0m     rdd, struct \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_createFromRDD(data\u001B[38;5;241m.\u001B[39mmap(prepare), schema, samplingRatio)\n\u001B[1;32m   1265\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1266\u001B[0m     rdd, struct \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_createFromLocal\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mmap\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mprepare\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mschema\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1267\u001B[0m jrdd \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jvm\u001B[38;5;241m.\u001B[39mSerDeUtil\u001B[38;5;241m.\u001B[39mtoJavaArray(rdd\u001B[38;5;241m.\u001B[39m_to_java_object_rdd())\n\u001B[1;32m   1268\u001B[0m jdf \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jsparkSession\u001B[38;5;241m.\u001B[39mapplySchemaToPythonRDD(jrdd\u001B[38;5;241m.\u001B[39mrdd(), struct\u001B[38;5;241m.\u001B[39mjson())\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:888\u001B[0m, in \u001B[0;36mSparkSession._createFromLocal\u001B[0;34m(self, data, schema)\u001B[0m\n\u001B[1;32m    880\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_createFromLocal\u001B[39m(\n\u001B[1;32m    881\u001B[0m     \u001B[38;5;28mself\u001B[39m, data: Iterable[Any], schema: Optional[Union[DataType, List[\u001B[38;5;28mstr\u001B[39m]]]\n\u001B[1;32m    882\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRDD[Tuple]\u001B[39m\u001B[38;5;124m\"\u001B[39m, StructType]:\n\u001B[1;32m    883\u001B[0m     \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    884\u001B[0m \u001B[38;5;124;03m    Create an RDD for DataFrame from a list or pandas.DataFrame, returns the RDD and schema.\u001B[39;00m\n\u001B[1;32m    885\u001B[0m \u001B[38;5;124;03m    This would be broken with table acl enabled as user process does not have permission to\u001B[39;00m\n\u001B[1;32m    886\u001B[0m \u001B[38;5;124;03m    write temp files.\u001B[39;00m\n\u001B[1;32m    887\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 888\u001B[0m     internal_data, struct \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_wrap_data_schema\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mschema\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    889\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sc\u001B[38;5;241m.\u001B[39mparallelize(internal_data), struct\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:861\u001B[0m, in \u001B[0;36mSparkSession._wrap_data_schema\u001B[0;34m(self, data, schema)\u001B[0m\n\u001B[1;32m    858\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(data)\n\u001B[1;32m    860\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m schema \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(schema, (\u001B[38;5;28mlist\u001B[39m, \u001B[38;5;28mtuple\u001B[39m)):\n\u001B[0;32m--> 861\u001B[0m     struct \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_inferSchemaFromList\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnames\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mschema\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    862\u001B[0m     converter \u001B[38;5;241m=\u001B[39m _create_converter(struct)\n\u001B[1;32m    863\u001B[0m     tupled_data: Iterable[Tuple] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mmap\u001B[39m(converter, data)\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:751\u001B[0m, in \u001B[0;36mSparkSession._inferSchemaFromList\u001B[0;34m(self, data, names)\u001B[0m\n\u001B[1;32m    749\u001B[0m infer_dict_as_struct \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jconf\u001B[38;5;241m.\u001B[39minferDictAsStruct()\n\u001B[1;32m    750\u001B[0m prefer_timestamp_ntz \u001B[38;5;241m=\u001B[39m is_timestamp_ntz_preferred()\n\u001B[0;32m--> 751\u001B[0m schema \u001B[38;5;241m=\u001B[39m \u001B[43mreduce\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    752\u001B[0m \u001B[43m    \u001B[49m\u001B[43m_merge_type\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    753\u001B[0m \u001B[43m    \u001B[49m\u001B[43m(\u001B[49m\u001B[43m_infer_schema\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrow\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnames\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minfer_dict_as_struct\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprefer_timestamp_ntz\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mrow\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    754\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    755\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m _has_nulltype(schema):\n\u001B[1;32m    756\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSome of types cannot be determined after inferring\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:753\u001B[0m, in \u001B[0;36m<genexpr>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m    749\u001B[0m infer_dict_as_struct \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jconf\u001B[38;5;241m.\u001B[39minferDictAsStruct()\n\u001B[1;32m    750\u001B[0m prefer_timestamp_ntz \u001B[38;5;241m=\u001B[39m is_timestamp_ntz_preferred()\n\u001B[1;32m    751\u001B[0m schema \u001B[38;5;241m=\u001B[39m reduce(\n\u001B[1;32m    752\u001B[0m     _merge_type,\n\u001B[0;32m--> 753\u001B[0m     (\u001B[43m_infer_schema\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrow\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnames\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minfer_dict_as_struct\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprefer_timestamp_ntz\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m row \u001B[38;5;129;01min\u001B[39;00m data),\n\u001B[1;32m    754\u001B[0m )\n\u001B[1;32m    755\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m _has_nulltype(schema):\n\u001B[1;32m    756\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSome of types cannot be determined after inferring\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/types.py:1441\u001B[0m, in \u001B[0;36m_infer_schema\u001B[0;34m(row, names, infer_dict_as_struct, prefer_timestamp_ntz)\u001B[0m\n\u001B[1;32m   1438\u001B[0m     items \u001B[38;5;241m=\u001B[39m \u001B[38;5;28msorted\u001B[39m(row\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__dict__\u001B[39m\u001B[38;5;241m.\u001B[39mitems())\n\u001B[1;32m   1440\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1441\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCan not infer schema for type: \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m \u001B[38;5;28mtype\u001B[39m(row))\n\u001B[1;32m   1443\u001B[0m fields \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m   1444\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m items:\n\n\u001B[0;31mTypeError\u001B[0m: Can not infer schema for type: <class 'int'>","errorSummary":"<span class='ansi-red-fg'>TypeError</span>: Can not infer schema for type: <class 'int'>","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n","\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)\n","File \u001B[0;32m<command-930677172032680>:1\u001B[0m\n","\u001B[0;32m----> 1\u001B[0m \u001B[43mspark\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcreateDataFrame\u001B[49m\u001B[43m(\u001B[49m\u001B[43mroll_num_list\u001B[49m\u001B[43m)\u001B[49m\n","\n","File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n","\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n","\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n","\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n","\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n","\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n","\u001B[1;32m     51\u001B[0m     )\n","\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n","\n","File \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1216\u001B[0m, in \u001B[0;36mSparkSession.createDataFrame\u001B[0;34m(self, data, schema, samplingRatio, verifySchema)\u001B[0m\n","\u001B[1;32m   1211\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_pandas \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data, pd\u001B[38;5;241m.\u001B[39mDataFrame):\n","\u001B[1;32m   1212\u001B[0m     \u001B[38;5;66;03m# Create a DataFrame from pandas DataFrame.\u001B[39;00m\n","\u001B[1;32m   1213\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m(SparkSession, \u001B[38;5;28mself\u001B[39m)\u001B[38;5;241m.\u001B[39mcreateDataFrame(  \u001B[38;5;66;03m# type: ignore[call-overload]\u001B[39;00m\n","\u001B[1;32m   1214\u001B[0m         data, schema, samplingRatio, verifySchema\n","\u001B[1;32m   1215\u001B[0m     )\n","\u001B[0;32m-> 1216\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_create_dataframe\u001B[49m\u001B[43m(\u001B[49m\n","\u001B[1;32m   1217\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mschema\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msamplingRatio\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mverifySchema\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# type: ignore[arg-type]\u001B[39;49;00m\n","\u001B[1;32m   1218\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n","\n","File \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1266\u001B[0m, in \u001B[0;36mSparkSession._create_dataframe\u001B[0;34m(self, data, schema, samplingRatio, verifySchema)\u001B[0m\n","\u001B[1;32m   1264\u001B[0m     rdd, struct \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_createFromRDD(data\u001B[38;5;241m.\u001B[39mmap(prepare), schema, samplingRatio)\n","\u001B[1;32m   1265\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n","\u001B[0;32m-> 1266\u001B[0m     rdd, struct \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_createFromLocal\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mmap\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mprepare\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mschema\u001B[49m\u001B[43m)\u001B[49m\n","\u001B[1;32m   1267\u001B[0m jrdd \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jvm\u001B[38;5;241m.\u001B[39mSerDeUtil\u001B[38;5;241m.\u001B[39mtoJavaArray(rdd\u001B[38;5;241m.\u001B[39m_to_java_object_rdd())\n","\u001B[1;32m   1268\u001B[0m jdf \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jsparkSession\u001B[38;5;241m.\u001B[39mapplySchemaToPythonRDD(jrdd\u001B[38;5;241m.\u001B[39mrdd(), struct\u001B[38;5;241m.\u001B[39mjson())\n","\n","File \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:888\u001B[0m, in \u001B[0;36mSparkSession._createFromLocal\u001B[0;34m(self, data, schema)\u001B[0m\n","\u001B[1;32m    880\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_createFromLocal\u001B[39m(\n","\u001B[1;32m    881\u001B[0m     \u001B[38;5;28mself\u001B[39m, data: Iterable[Any], schema: Optional[Union[DataType, List[\u001B[38;5;28mstr\u001B[39m]]]\n","\u001B[1;32m    882\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRDD[Tuple]\u001B[39m\u001B[38;5;124m\"\u001B[39m, StructType]:\n","\u001B[1;32m    883\u001B[0m     \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n","\u001B[1;32m    884\u001B[0m \u001B[38;5;124;03m    Create an RDD for DataFrame from a list or pandas.DataFrame, returns the RDD and schema.\u001B[39;00m\n","\u001B[1;32m    885\u001B[0m \u001B[38;5;124;03m    This would be broken with table acl enabled as user process does not have permission to\u001B[39;00m\n","\u001B[1;32m    886\u001B[0m \u001B[38;5;124;03m    write temp files.\u001B[39;00m\n","\u001B[1;32m    887\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n","\u001B[0;32m--> 888\u001B[0m     internal_data, struct \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_wrap_data_schema\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mschema\u001B[49m\u001B[43m)\u001B[49m\n","\u001B[1;32m    889\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sc\u001B[38;5;241m.\u001B[39mparallelize(internal_data), struct\n","\n","File \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:861\u001B[0m, in \u001B[0;36mSparkSession._wrap_data_schema\u001B[0;34m(self, data, schema)\u001B[0m\n","\u001B[1;32m    858\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(data)\n","\u001B[1;32m    860\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m schema \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(schema, (\u001B[38;5;28mlist\u001B[39m, \u001B[38;5;28mtuple\u001B[39m)):\n","\u001B[0;32m--> 861\u001B[0m     struct \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_inferSchemaFromList\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnames\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mschema\u001B[49m\u001B[43m)\u001B[49m\n","\u001B[1;32m    862\u001B[0m     converter \u001B[38;5;241m=\u001B[39m _create_converter(struct)\n","\u001B[1;32m    863\u001B[0m     tupled_data: Iterable[Tuple] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mmap\u001B[39m(converter, data)\n","\n","File \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:751\u001B[0m, in \u001B[0;36mSparkSession._inferSchemaFromList\u001B[0;34m(self, data, names)\u001B[0m\n","\u001B[1;32m    749\u001B[0m infer_dict_as_struct \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jconf\u001B[38;5;241m.\u001B[39minferDictAsStruct()\n","\u001B[1;32m    750\u001B[0m prefer_timestamp_ntz \u001B[38;5;241m=\u001B[39m is_timestamp_ntz_preferred()\n","\u001B[0;32m--> 751\u001B[0m schema \u001B[38;5;241m=\u001B[39m \u001B[43mreduce\u001B[49m\u001B[43m(\u001B[49m\n","\u001B[1;32m    752\u001B[0m \u001B[43m    \u001B[49m\u001B[43m_merge_type\u001B[49m\u001B[43m,\u001B[49m\n","\u001B[1;32m    753\u001B[0m \u001B[43m    \u001B[49m\u001B[43m(\u001B[49m\u001B[43m_infer_schema\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrow\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnames\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minfer_dict_as_struct\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprefer_timestamp_ntz\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mrow\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n","\u001B[1;32m    754\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n","\u001B[1;32m    755\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m _has_nulltype(schema):\n","\u001B[1;32m    756\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSome of types cannot be determined after inferring\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n","\n","File \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:753\u001B[0m, in \u001B[0;36m<genexpr>\u001B[0;34m(.0)\u001B[0m\n","\u001B[1;32m    749\u001B[0m infer_dict_as_struct \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jconf\u001B[38;5;241m.\u001B[39minferDictAsStruct()\n","\u001B[1;32m    750\u001B[0m prefer_timestamp_ntz \u001B[38;5;241m=\u001B[39m is_timestamp_ntz_preferred()\n","\u001B[1;32m    751\u001B[0m schema \u001B[38;5;241m=\u001B[39m reduce(\n","\u001B[1;32m    752\u001B[0m     _merge_type,\n","\u001B[0;32m--> 753\u001B[0m     (\u001B[43m_infer_schema\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrow\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnames\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minfer_dict_as_struct\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprefer_timestamp_ntz\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m row \u001B[38;5;129;01min\u001B[39;00m data),\n","\u001B[1;32m    754\u001B[0m )\n","\u001B[1;32m    755\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m _has_nulltype(schema):\n","\u001B[1;32m    756\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSome of types cannot be determined after inferring\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n","\n","File \u001B[0;32m/databricks/spark/python/pyspark/sql/types.py:1441\u001B[0m, in \u001B[0;36m_infer_schema\u001B[0;34m(row, names, infer_dict_as_struct, prefer_timestamp_ntz)\u001B[0m\n","\u001B[1;32m   1438\u001B[0m     items \u001B[38;5;241m=\u001B[39m \u001B[38;5;28msorted\u001B[39m(row\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__dict__\u001B[39m\u001B[38;5;241m.\u001B[39mitems())\n","\u001B[1;32m   1440\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n","\u001B[0;32m-> 1441\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCan not infer schema for type: \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m \u001B[38;5;28mtype\u001B[39m(row))\n","\u001B[1;32m   1443\u001B[0m fields \u001B[38;5;241m=\u001B[39m []\n","\u001B[1;32m   1444\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m items:\n","\n","\u001B[0;31mTypeError\u001B[0m: Can not infer schema for type: <class 'int'>"]}}],"execution_count":0},{"cell_type":"code","source":["spark.createDataFrame(roll_num_list, 'int').show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"d3c4e03a-e5be-48cd-ad5a-eb19d13586f7","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-----+\n|value|\n+-----+\n|   10|\n|   20|\n|   23|\n|   25|\n+-----+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.types import IntegerType\nspark.createDataFrame(roll_num_list, IntegerType())"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"0c19d32e-1efc-43d4-aa42-bab3c8402bfc","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Out[12]: DataFrame[value: int]"]}],"execution_count":0},{"cell_type":"code","source":["student_names_list = ['Allen', 'Donald', 'Mark']"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"22944bdd-e9b4-41ba-b084-66b16148ca8e","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["spark.createDataFrame(student_names_list, 'string')"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"1a563b0f-f818-4915-9e02-335618f2e9a1","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Out[17]: DataFrame[value: string]"]}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.types import StringType\n\nspark.createDataFrame(student_names_list, StringType()).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"dec16158-5fea-49bf-aa31-4271e52d156f","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+------+\n| value|\n+------+\n| Allen|\n|Donald|\n|  Mark|\n+------+\n\n"]}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"01_Create_Single_Col_Spark_Df_using_List","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{}}},"nbformat":4,"nbformat_minor":0}
